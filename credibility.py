# -*- coding: utf-8 -*-
"""
Deliverable 2 — Balanced Technical Review
=========================================

This file provides section-level comments that highlight both strengths and areas to improve. 
The comments are written in a straightforward style, reflecting a student learning data science.

Conventions:
- STRENGTHS: Positive feedback on what works well.
- IMPROVEMENTS: Suggestions on what could be better.
- NOTES: Extra context for understanding.

"""
# -*- coding: utf-8 -*-
"""Deliverable1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9OwtqzUa71JuXTcZi_77oq31tuomPDy
"""

import argparse
import re
import math
from urllib.parse import urlparse
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error

# -------------------------------
# Domain priors (hand-tuned)
# -------------------------------
# These are starting beliefs about a site's credibility, on [0,1].
# We bias towards reputable journals, government and major outlets.
# NOTE: This is not exhaustive—it's a simple demo list for Deliverable 1.
DOMAIN_PRIORS = {
    "nature.com": 0.95,
    "science.org": 0.95,
    "sciencemag.org": 0.95,
    "nejm.org": 0.95,
    "thelancet.com": 0.95,
    "sciencedirect.com": 0.95,
    "arxiv.org": 0.80,
    "doi.org": 0.90,
    "nih.gov": 0.95,
    "ncbi.nlm.nih.gov": 0.95,
    "cdc.gov": 0.95,
    "who.int": 0.90,
    "reuters.com": 0.92,
    "www.reuters.com": 0.92,  # safety in case normalization fails
    "bbc.com": 0.90,
    "www.bbc.com": 0.90,       # safety in case normalization fails
    "nytimes.com": 0.88,
    "theguardian.com": 0.85,
    "wikipedia.org": 0.85,
    "medium.com": 0.60,
    "substack.com": 0.55,
}

# Detect DOI patterns in URLs (helps identify academic sources)
DOI_RE = re.compile(r"10\.\d{4,9}/[-._;()/:A-Za-z0-9]+")

# -------------------------------
# URL helpers
# -------------------------------
def _normalize_host(netloc: str) -> str:
    """Normalize a URL host so it matches our DOMAIN_PRIORS keys.
    - Lowercase
    - Strip credentials, port, trailing dot
    - Remove common subdomain prefixes like www., m., amp.
    """
    host = netloc.lower().strip()
    if "@" in host:  # strip credentials if present
        host = host.split("@", 1)[-1]
    if ":" in host:  # strip port if present
        host = host.split(":", 1)[0]
    if host.endswith("."):  # strip trailing dot
        host = host[:-1]
    for prefix in ("www.", "m.", "amp."):
        if host.startswith(prefix):
            host = host[len(prefix):]
    return host

def _registrable(host: str) -> str:
    """Return the registrable domain (e.g., 'sub.example.com' -> 'example.com')."""
    parts = [p for p in host.split(".") if p]
    if len(parts) >= 2:
        return ".".join(parts[-2:])
    return host

def _lookup_prior(host: str):
    """Find the best-matching prior for a host.
    Tries exact host, then registrable (example.com), then longest suffix match.
    """
    if host in DOMAIN_PRIORS:
        return DOMAIN_PRIORS[host]
    reg = _registrable(host)
    if reg in DOMAIN_PRIORS:
        return DOMAIN_PRIORS[reg]
    best_key = None
    for key in DOMAIN_PRIORS.keys():
        if host.endswith(key):
            if best_key is None or len(key) > len(best_key):
                best_key = key
    return DOMAIN_PRIORS.get(best_key) if best_key else None

def _is_valid_url(url: str) -> bool:
    """Basic URL validation: must be http(s) and have a host."""
    try:
        p = urlparse(url)
        return p.scheme in ("http", "https") and bool(p.netloc)
    except:
        return False

# -------------------------------
# SECTION: Rule-based scoring
# STRENGTHS: Rules like HTTPS or DOI make the system interpretable and easy to understand.
# IMPROVEMENTS: The weights for rules should be tested and adjusted using data.
# Rule-based scoring
# -------------------------------
def evaluate_source(url: str, debug: bool=False):

    """Compute a credibility score using simple, transparent rules.
    Returns: {'score': float, 'explanation': str}
    """
    # Reject bad inputs early
    if not isinstance(url, str) or not url.strip():
        return {"score": 0.0, "explanation": "Invalid input"}
    if not _is_valid_url(url):
        return {"score": 0.0, "explanation": "Invalid URL"}

    parsed = urlparse(url)
    raw_host = parsed.netloc
    host = _normalize_host(raw_host)
    reg = _registrable(host)

    # Start from a *moderately conservative* base for any valid URL
    # Lowering this base to 0.6 means untrusted or unknown sites won’t look too high by default. I originally had it at .7 and the untrusted sites were still receiving a decent score.
    score = 0.6
    signals = ["Base=0.6"]

    # HTTPS is a small but positive signal (transport security)
    if parsed.scheme == "https":
        score += 0.05
        signals.append("HTTPS (+0.05)")
    else:
        signals.append("Non-HTTPS (no bonus)")

    # Domain prior: our strongest signal. Give it higher weight than the base and other add-ons.
    prior = _lookup_prior(host)
    if prior is not None:
        score = 0.8 * prior + 0.2 * score  # 80% prior, 20% everything else so far
        signals.append(f"Domain prior={prior} (80% weight)")

    # DOI signal: many academic sources live under doi.org links
    if "doi.org" in host or "doi.org" in reg or DOI_RE.search(url):
        score += 0.15
        signals.append("DOI detected (+0.15)")

    # TLD bonuses: educational and government domains often have stronger editorial standards
    if host.endswith(".edu") or reg.endswith(".edu"):
        score += 0.10
        signals.append(".edu (+0.10)")
    if host.endswith(".gov") or reg.endswith(".gov"):
        score += 0.10
        signals.append(".gov (+0.10)")

    # Clip to [0,1] to avoid overshooting
    score = max(0.0, min(1.0, score))

    if debug:
        signals.append(f"[debug host={raw_host} -> {host} reg={reg}]")

    return {"score": round(score, 4), "explanation": "; ".join(signals)}

# -------------------------------
# ML component (prototype)
# -------------------------------
# We train a very simple Linear Regression to mimic/augment the rule-based score
# using TF-IDF features of sentence text. This is a placeholder to demonstrate
# a hybrid approach; with small toy datasets, R^2 can be poor (that’s okay here).
SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\s+')

def split_sentences(text: str):
    """Naive sentence splitter: splits on punctuation + space."""
    if not isinstance(text, str):
        return []
    return [p.strip() for p in SENT_SPLIT_RE.split(text.strip()) if p.strip()]

def build_sentence_table(df: pd.DataFrame) -> pd.DataFrame:
    """Explode each article's text into sentences (id, sentence)."""
    rows = []
    for _, row in df.iterrows():
        for s in split_sentences(str(row.get("text", ""))):
            rows.append({"id": row["id"], "sentence": s})
    return pd.DataFrame(rows)

def aggregate_sentence_vectors(sent_df, vec):
    """Vectorize sentences and average per article id to get a single vector per id."""
    if sent_df.empty:
        return pd.DataFrame(columns=["id", "agg_vec"]).set_index("id")
    X = vec.fit_transform(sent_df["sentence"])
    ids = sent_df["id"].values
    agg = {}
    for i, aid in enumerate(ids):
        row_vec = X.getrow(i).toarray().ravel()  # safe on Colab
        agg.setdefault(aid, []).append(row_vec)
    agg = {aid: np.mean(vectors, axis=0) for aid, vectors in agg.items()}
    return pd.DataFrame({"id": list(agg.keys()), "agg_vec": list(agg.values())}).set_index("id")


def attach_rule_scores(df: pd.DataFrame, debug: bool=False) -> pd.DataFrame:

    """Compute rule-based scores (one per row). If a 'citations' column exists,
    we apply a log-based boost up to +0.20 to simulate academic citation strength.
    """
    df = df.copy()
    scores = []
    boosts = []
    for _, row in df.iterrows():
        res = evaluate_source(str(row.get("url", "")), debug=debug)
        s = float(res.get("score", 0.0))

        # Optional: citation boost for academic items if the CSV includes 'citations'
        b = 0.0
        if "citations" in df.columns:
            # Use log1p so additional citations have diminishing returns; cap at +0.20
            try:
                c = float(row.get("citations", 0))
                if np.isfinite(c) and c > 0:
                    b = float(min(0.2, math.log1p(c) / 100.0))
                    s = max(0.0, min(1.0, s + b))
            except Exception:
                b = 0.0

        scores.append(s)
        boosts.append(b)


    df["rule_score"] = scores
    if "citations" in df.columns:
        df["citation_boost"] = np.round(boosts, 4)
    return df

def train_linear_regression(X, y):
    """Train Linear Regression and report simple metrics (prototype-level)."""
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=42)
    model = LinearRegression().fit(Xtr, ytr)
# SECTION: Predictions
# STRENGTHS: Model is set up to make predictions which is the goal of the system.
# IMPROVEMENTS: Add more evaluation metrics like precision, recall, and ROC-AUC to check performance.
    yp = model.predict(Xte)
    return model, {"r2": r2_score(yte, yp), "mae": mean_absolute_error(yte, yp)}

# -------------------------------
# CLI entrypoint
# -------------------------------
# SECTION: Main function
# STRENGTHS: Nice to have a clear entry point for the program.
# IMPROVEMENTS: Could split the workflow into smaller functions for clarity.
def main():
    parser = argparse.ArgumentParser(description="Hybrid credibility scoring (rule-based + ML).")
    parser.add_argument("--input", required=True, help="CSV with columns: id,url,text[,citations]")
    parser.add_argument("--output", default="hybrid_scores.csv", help="Output CSV path")
    parser.add_argument("--alpha", type=float, default=0.7,
                        help="Hybrid weight: alpha*rule + (1-alpha)*ml (default 0.7)")
    parser.add_argument("--debug", action="store_true", help="Include debug hints in explanations")
    args = parser.parse_args()

    # 1) Load data
    df = pd.read_csv(args.input)
    for col in ["id", "url", "text"]:
        if col not in df.columns:
            raise ValueError(f"Missing required column: {col}")

    df = attach_rule_scores(df, debug=args.debug)

    # 3) Build sentence vectors (TF-IDF on sentences, then average per id)
    sent_df = build_sentence_table(df)
    vec = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.95)
    agg = aggregate_sentence_vectors(sent_df, vec)

# SECTION: Predictions
# STRENGTHS: Model is set up to make predictions which is the goal of the system.
# IMPROVEMENTS: Add more evaluation metrics like precision, recall, and ROC-AUC to check performance.
    # 4) Train simple regression to predict rule scores from text vectors
    merged = df.set_index("id").join(agg, how="inner")
    X = np.vstack(merged["agg_vec"].values)

    y = merged["rule_score"].values
    model, metrics = train_linear_regression(X, y)
    
    ml_pred = np.clip(model.predict(X), 0, 1)

# SECTION: Rule-based scoring
# STRENGTHS: Rules like HTTPS or DOI make the system interpretable and easy to understand.
# IMPROVEMENTS: The weights for rules should be tested and adjusted using data.
    # 5) Blend the two: hybrid = alpha*rule + (1-alpha)*ml
    alpha = float(np.clip(args.alpha, 0.0, 1.0))
    hybrid = np.clip(alpha * y + (1 - alpha) * ml_pred, 0, 1)

    cols = ["id", "url", "rule_score"]
    if "citation_boost" in merged.reset_index().columns:
        cols.append("citation_boost")
    out = merged.reset_index()[cols]
    out["ml_pred"] = np.round(ml_pred, 4)
    out["hybrid_score"] = np.round(hybrid, 4)
    out.to_csv(args.output, index=False)

    # 7) Log summary
    print(f"Wrote {args.output} with {len(out)} rows.")
    print(f"Metrics: R2={metrics['r2']:.3f}, MAE={metrics['mae']:.3f}")
    print(f"Hybrid score = alpha*rule + (1-alpha)*ml_pred (alpha={alpha})")
    if args.debug:
        print("Debug mode: host normalization and signals are included in explanations via evaluate_source().")

if __name__ == "__main__":
    main()
# ------------------------------------------------------------------
# Overall Review Summary
# ------------------------------------------------------------------
# STRENGTHS: The code shows the building of a credibility scoring system. 
# It combines simple rules with a machine learning model, which is a smart hybrid approach. 
# The workflow is easy to follow, and the main pieces (features, model, evaluation) are all there.
#
# IMPROVEMENTS: The model choice can be improved (Logistic instead of Linear Regression), 
# evaluation can be more thorough (cross-validation, ROC-AUC, calibration), and rule weights 
# should be tuned with data. Future versions could also test embeddings for better text 
# understanding. Saving results and models for reproducibility would also be good practice.
#
# This is a great foundation and shows strong progress for someone building their skills in 
# data science. With these next steps, it can become a much stronger system.
# ------------------------------------------------------------------
